\chapter{Floating point}
To deal with non integer numbers, two different approaches can be exploited. In
the first case fixed-point notation is employed so all integer arithmetic units
already seen can be reused with very small changes, in the second case
floating-point is used leading a radically different architecture.

\section{Floating point representation}
In order to represent a number using floating-point notation, 3 fields have to be defined:
\begin{enumerate}
  \item Sign.
  \item Exponent.
  \item Significant.
\end{enumerate}
As a first example:
$$+1.6 \cdot 2^{-3} \rightarrow 0.2_{10}$$

According to the amount of bits allocated for the exponent and significant,
different standards are available. The most common is IEEE 754-1985, in which
are defined:
\begin{itemize}
  \item Single precision(32 bits):
    \subitem 1 bit sign.
    \subitem 8 bit exponent.
    \subitem 23 bit fractional part (which is not actually equal to the significant).

  \item Double precision (64 bits):
    \subitem 1 bit sign.
    \subitem 11 bit exponent.
    \subitem 52 bit fractional part.
\end{itemize}

According to this standard \textit{exponent} is the biased one, meaning that we
have to obtain the biased exponent to be used in the following way (it's like a
shifted representation):
$$bias exp= unbias exp + 127 $$

Instead the \textit{fractional part} is an unsigned number which represents the
part of the significant placed at the right of the decimal point, since the
significant is almost always something like:
$$significand=1.fractional part$$
Because MSB of significant is always 1 there is no needed to save it, provided
that the value is represented in canonical way.

\subparagraph{Example}
The number is given as:
\begin{verbatim}
|1 |10000001|010 .... 0|
|s|  exp    |f.p. (23 bit)|
\end{verbatim}
where:

\begin{itemize}
  \item Sign bit equal to  1 $\rightarrow$ minus.
  \item $Exp(biased) = 10000001_2 = 129_{10}$.
  \item Exp(unbiased) = real exponent= 129-127=2.
  \item Fractional part=0.25.
  \item Significant    =1.25.
\end{itemize}

So the final value will be $$-1.25 2^2 = (-5)_{10}$$.

\subsection{Rounding}
Assuming to multiply two values $s_1, s_2$, the significant part of the product
will be $s_1 s_2 = s$, since the starting operands are represented using 23
bits then the product should have a 46 bit significant part which is not
suitable for the previously defined standard. To overcome this problem some
rounding has to be performed, in particular IEEE standard expects rounding to
nearest even. For the case in which the number to be rounded is exactly in the
middle, let's see the following example.

\subparagraph{Example (base 10) - mid value rounding}
We want to compute $6.1 \cdot 0.5 =3.05$, to round the result two possibilities
are available: 3.0 and 3.1.
From the implementation point of view it would be easier choose the higher one
(3.1) leading to nearest/up solution, however we want to use the nearest to
even, so we always select the choice having LSB equal to zero (i.e. lowest
approximation).

\subsection{Special values}
\begin{itemize}
  \item Division by 0 leads to  $\pm \infty$.
  \item $\sqrt{-1} \rightarrow NaN$.
  \item De-normal numbers are the ones for which $E_{min} \leq exp \leq
  E_{max}$. Usually a number can be represented by $1.00...0 2^{E_{min}}$, in
  IEEE standard $E_{min}=-127$ and this should be the lowest value
  represented. However we can represent lower values by accepting that they
  are no more in the canonical form, so instead of being like $1.000$ they
  are $0.1000$, by doing that we can continue to divide the number by 2
  shifting the original 1 to the right. This technique allows to extend a
  little bit toward the bottom the representation range.
\end{itemize}

\subsection{IEEE754}

\begin{verbatim}
img55

















\end{verbatim}

\section{Floating-point multiplication}

\# copiare primi 5 min

\begin{verbatim}
\ example
\# vedi quaderno


















\end{verbatim}


In order to easily analyze rounding technique, just consider a shortest
representation where bits allocated for the significant are less than the one
expected by single precision (23 bits). Wanting to perform the multiplication
between 5-bits operands, the result will be on 10 bits; the two MSB of the
result represent the integer part while 8 LSB are the fractional part.
\begin{eqnarray*}
s_1=(1). \sqcup \sqcup \sqcup \sqcup\\
s_2=(1). \sqcup \sqcup \sqcup \sqcup\\
s_1 \cdot s_2= \sqcup \sqcup . \sqcup \sqcup \sqcup \sqcup \sqcup \sqcup \sqcup \sqcup
\end{eqnarray*}

Some cases have to be considered:
\begin{enumerate}
\item Operands are in normal representation so the integer part cannot be
greater or equal to 2, this means that an overflow condition occurs if the
result is something like $1\sqcup.\sqcup \sqcup \sqcup \sqcup \sqcup \sqcup
\sqcup \sqcup$. In this case the MSB of integer part becomes the integer part
itself and we have to take the 4 following bits as the fractional part (the
first one of them was the LSB of integer part), this corresponds to perform a
right shift and to increment the exponent by 1. This implies that the rounding
operation is based on the value of the fourth bit in original significant part
(i.e. the first bit to be canceled out).

\item No overflow occurs, so the result is something like $0 1.|\sqcup \sqcup \sqcup \sqcup| \sqcup \sqcup \sqcup \sqcup$. Since fractional part has to stay in 4 bits, some rounding has to be performed. Looking at the first bit to be discharged (i.e. fifth bit) and depending on the fact that it is equal or not to 0, we will have to add or not 1 to the new fractional part. All other bits that have to be discharged don't need to be computed, however this is a rounding-up operation, which is not exactly what IEEE is asking for.
\end{enumerate}

In these two cases we have to test two different bit for the rounding, moreover starting from no overflow condition and adding a +1 for rounding it is not impossible that the result goes in overflow. Overflow after rounding (which is actually a third condition) requires to shift right and increment the exponent like in the first case.

\subparagraph{Example: rounding}

Looking at some examples, assuming that the number on the left refers to the un-rounding result:
\begin{eqnarray*}
1.0100 0111 \rightarrow 1.0100\\
1.0100 1001 \rightarrow 1.0101\\
1.0100 1000 \rightarrow 1.0101
\end{eqnarray*}

In the first example, since the $5^{th}$ bit in the original fractional part is equal to be zero the rounded result just consists of the 4 MSB of the fractional part (no need to add +1). In the second case rounding to the nearest requires to add +1; in the third case (which is exactly half way condition) depending on the chosen rounding technique different bits have to be considered, since for the rounding to the nearest up approach we just look at $5^{th}$ bit (the final result is therefore 1.0101) while for the rounding to nearest even we have to look at the remaining bits.

\subsection{Rounding to nearest-even}
To understand better how it works, let's consider the following number to be rounded:
$$X=1.x_1 x_2 x_3 x_4 x_5 x_6 x_7 x_8$$
Defining as sticky bit $$sticky= x_6 + x_7 + x_8$$ (OR of 3 LSB of fractional
part) we have:

\begin{center}
  \begin{tabular}{|c|c|c|}
    \hline
    Unrounded&  Rounding to nearest even& Sticky bit\\
    \hline
    1.0100 0|111|&  1.0100&         1\\
    1.0100 1|001|&  1.0101&         1\\
    1.0100 1|000|&  1.0100&         0\\
    \hline
  \end{tabular}
\end{center}

In the last case the addition +1 is not performed although $5^{th}$ bit is equal to 1 since it is the half way condition.
Instead of calculating the sticky bit using the OR logic function (which may affect the critical path), we can try to anticipate the value of the sticky bit looking at the operands.

\subparagraph{Trailing zeros}

We want to compute the following product:

$$XY=P$$

Looking at the amount of trailing zeros (i.e. \# of 0s in sequence starting
from LSB positions of the fractional part), if $t_{zx}=$ \# trailing zeros in X
and $t_{zy}=$ trailing zeros in Y, then $t_{zp}=t_{zx}+t_{zy}$. This
means that the amount of trailing zeros in the product is known a priori. So
for example a possible situation is the following:

$$ 0.1100 \cdot 0.1000 \longrightarrow  \sqcup \sqcup . \sqcup \sqcup \sqcup
00000
$$

So if $t_{zp}$ is shorter than the numbers of bits to be discharged this
implies that sticky bit is equal to 1, otherwise  0: we can compute the sticky
bit in parallel to the multiplication, no needed to wait for the result.\\

Some possible architectures are:

\begin{verbatim}
img 60
















\end{verbatim}

We have not considered special cases (for the input or the output), but apart
these (which will be handled by additional parts), a general architecture for
the floating point multiplication is:

\begin{verbatim}
img 61



























\end{verbatim}

\section{Floating point division}

To perform a floating point division a very similar architecture can be
employed, with the difference that we have to perform the subtraction of
exponents and the integer division of fractional parts. All the remaining
stuffs are almost the same. This is no more true for addition and subtractions
since a completely different architecture is needed.

\section{Floating point addition}

\subparagraph{Starting example (in base 10)}
Assuming we want to perform the following addition:

$$S=99.5+0.8$$

The required steps are:
\begin{enumerate}
  \item Write in canonical representation the input data, so:
   $$ S= 9.95 \cdot 10^1 + 8.0 \cdot 10 ^{-1} $$
  \item Alignment of exponents since maybe we need to shift one of the two
  numbers to obtain the same exponent for the two operands (if needed). We
  are temporarily de-normalizing the operand with the smallest exponent,
  obtaining:
  $$ 8.0 \cdot 10^{-1} \longrightarrow 0.08 \cdot 10^1 $$
  \item Add together two significants:
  $$ 99.5+0.08= 10.03 $$
  \item Normalize the result:
  $$ 10.03 \cdot 10^1  \longrightarrow 1.003 \cdot 10^2 $$
\end{enumerate}

\subparagraph{Example in quarter precision format}

In this representation we use 8 bit overall, meaning that:

\begin{itemize}
  \item Bias for the exponent is equal to 3.
  \item Number of bits for sign = 1.
  \item Number of bits for exponent = 3.
  \item Number of bits for fractional part = 4.
\end{itemize}

We want to perform:

$$S= 7.5 + 0.5$$

so the starting operands are represented as:

$$7.5 \longrightarrow 1.875 \cdot 2^2 \longrightarrow |0|101|(1).1110$$
$$0.5 \longrightarrow 1.0  \cdot 2^{-1} \longrightarrow  |0|010|(1).0000$$

Reproducing the previous steps:

\begin{enumerate}
\item Denormalization:

  $$|0|010|(1).0000 \rightarrow 0|101|(0).0010 $$

\item Add together significants:
  $$(0).0010 + (1).1110 = (10).0000$$
  An overflow occurs since we need 2 bit to correctly representing the
  integer part.

\item Normalization, due to the overflow we need to perform a right shift and
add 1 at the exponent:
  $$ 0|101|(10).0000 \rightarrow 0|110|(1).0000$$
\end{enumerate}

We always need a correct representation for the integer part although we are
not going to store it anywhere.

\begin{verbatim}
img 62

























\end{verbatim}

Basing on the sign of $exp1-exp2$ we determine which operand has to be
denormalized in order to align them properly. Because the significant with the
lowest exponent has to be shifted of an amount equal to $exp1-exp2$ a barrel
shifter is required since $exp1-exp2$ is variable.
Instead if a rounding operation is required, we can reuse what we have already
seen for the multiplication.
This architecture is nice if the two operands are positive, but if we want to
perform a subtraction (or an addition where one operand is negative) we have to
substitute the significant adder with a substractor by exploiting a CA2 block.
Except this last point, right now addition and subtraction seem to be quite
close, however the following phenomenon may occur.

\subsection{Cancellation}
Let's assume that we want to perform the following operation:

\begin{verbatim}
 +  1 . 0 0 0 0 0
- 0 . 0 0 0 0 0 1 0 1 1 1 1
------------------------------
\end{verbatim}

By taking the CA2 of second operand it corresponds to:
\begin{verbatim}
  1 . |0 0 0 0 0|
 +  1 . |1 1 1 1 1| 0 1 0 0 0 1
--------------------------------
    0 .  1 1 1 1 1
\end{verbatim}

It may happen that due to cancellation the integer part of result becomes 0, so
starting from two operands which are in normal form the result is in denormal
form: now to compensate it a left shift and a decrement of the exponent is
required (opposite case with respect to overflow in the addition). In this
example cancellation is limited to the integer part, but in principle we may
have cancellation in other positions, this means that in general we may need a
generic amount of left shift and exponent decrement to be performed. This leads
to a the introduction of further complexity in adders.

\begin{verbatim}
img 63
















\end{verbatim}

To control the barrel shifter (which performs the right/left shifts) we need to
compute the amount of 0 in MSB position that corresponds to the number of
leading zeros.
This extra unit is in the critical path exactly as before so what we want is a
a leading zero estimator which drives control signal for the barrel shifter
working in parallel to the adder. \\

Another improvement can be derived looking at the possible cases:

\begin{eqnarray*}
A+B\\
A+B+1\\
A-B\\
A-B-1\\
B-A\\
B-A-1\\
\end{eqnarray*}

Depending on the sign of operands and the fact that we may have or not
rounding, in principle 6 cases should be handled. Actually they can be
compacted by noticing that:

$$A-B= A + \overline{B} +1$$
$$A-B-1 = A + \overline{B}$$
$$B-A= B - A +1-1= -(A-B-1)-1= \overline{A-B-1} +1-1= \overline{A+ \overline{B}}$$
$$B-A-1= \overline{A+ \overline{B}+1}$$


So:

\begin{verbatim}
img 64
















\end{verbatim}

\begin{center}
  \begin{tabular}{|l|c|c|}
    \hline
     &    $\alpha$&   $\beta$ \\
    \hline
    A+B&    0&      0\\
    A-B&    1&      0\\
    B-A&    1&      1\\
    \hline
  \end{tabular}
\end{center}

\#img architettura completa (img proiettata)

A couple of simplifications/improvements can be exploited: with the compounder
adder we can compute in parallel all possible combinations of input data (sum
and subtraction of A and B) and the correction for rounding. This is the case
of the last shown architecture. Also the leading zero logic is along the
critical path: instead of taking the adder output we can have a logic that
starting from adder inputs is able to compute the number of leading zeros (like
for the multiplier). However estimate/compute the number of leading zero is
much more complicated than the case of trailing zeros. The exact computation of
leading zeros cannot be performed starting from the operands since only an
estimation can be evaluated.

\subsection{Leading zeros estimation}
Let's take A,B as two input operands (where significants have already been
shifted), in case of sum cancellation is not a problem since it does not
happen, so focusing on subtraction two cases may occur:

\begin{itemize}
  \item $$A \geq B \longrightarrow A'=A; B'=\bar{B} \longrightarrow
  S=A'+B'+A=A-B$$
  \item $$B \geq A \longrightarrow A'=\bar{A}; B'=B \longrightarrow
  S=A'+B'+A=B-A$$
\end{itemize}

To estimate the amount of leading zeros the following formula can be employed:

$$E_i = A'_i \oplus B'_i \cdot (A'_{i-1}+B'_{i-1})$$

Where $E_i$ gives the numbers of leading zeros with a good approximation.

\subparagraph{Example}
Let's take:

\begin{verbatim}
A 1000  0000  1010  0001
B 0111  1111  0110  1001
\end{verbatim}

We want to compute A-B, so
\begin{verbatim}
A'  1000  0000  1010  0001
B'  1000  0000  1001  0110

E 0000  0001  0100  100-

S 0000  0001  0011  1000
\end{verbatim}

A part for LSB, the position of the first '1' starting from MSB  is exactly the
same, meaning that the estimation is correct. Also if the estimation is wrong,
the error is always one position on the left (so the position of the first 1 is
one less than the actual one):

\begin{verbatim}
A 1001  1100  1010  0001
B 1001  1100  1000  1001
B'  0110  0011  0111  0110
E 0000  0000  0010  100-
S
\end{verbatim}

If this block works in parallel to the adder than the shifter works on the
prediction of this logic, so it does almost of the work. If estimation is not
correct, a compensation shifter is placed so that using the actual adder output
we are able to perform the extra shift required.

\begin{verbatim}
img 70










\end{verbatim}

This is a modular structure where each block is connected to the previous and
to the next.

\subsection{Barell shifter}
Many solutions can be employed to implement a barrel shifter:

\begin{enumerate}
  \item \textit{Direct solution}
  \begin{verbatim}
  img 71






  \end{verbatim}

  we need k multiplexers each one having k-inputs.

  \item \textit{Logarithmic solution}
  By adopting a multiple layers approach:

  \begin{verbatim}
  img 72
















  \end{verbatim}

  \begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    $s_1$&  $s_2$& $O_3$& $O_2$& $O_1$& $O_0$ \\
    \hline
    0&  0&    $I_3$&  $I_2$&  $I_1$&  $I_0$\\
    0&  1&    $I_0$&  $I_3$&  $I_2$&  $I_1$\\
    1&  0&    $I_1$&  $I_0$&  $I_3$&  $I_2$\\
    \hline
    \end{tabular}
  \end{center}
  The complexity is $k log_2 k$ much lower than $k^2$ in direct solution.
\end{enumerate}
