\chapter{Dividers}
The aim is to evaluate quotient and remainder starting from two input data, by calling:
\begin{itemize}
  \item $z$: dividend (2k bits).
  \item $d$: divisor  (k bits).
  \item $q$: quotient (k bits).
  \item $r$: remainder  (k bits). % il remainder è r o s???
\end{itemize}

The outputs will be:
$$\frac{z}{d}=q+(\frac{r}{d})$$
$$r=z-dq$$

\section{Pen and paper approach}

Let's assume $k=4$ bit:

\begin{verbatim}
img44











\end{verbatim}

Each partial product experiences a different shift, to avoid the allocation of a barrel shifter we can exploit the same idea of the multiplier, i.e. shift everything by $2^k$ and then shift only by one position at the time the partial remainder, in this way 2 fixed-amount shifters are sufficient.

$$s^{(0)}=z $$ 
$$ s^{(j)}=2s^{(j-1)}-q_{k-j}d2^k$$

Relative position of partial remainder is the same as in the pen and paper algorithm, in the last step we have to:
$$s^{(0)}=s2^k$$

\subparagraph{Example}
By considering $k=4$, $z=(117)_{10}=(01110101)_2$ and $d=(10)_{10}=(00001010)_2$:

\begin{verbatim}
ex























\end{verbatim}

The architecture can be simplify as follows:

\begin{verbatim}
img45














\end{verbatim}

The critical point is the data dependency we have in these steps, in particular we cannot save the partial remainder before testing $C_{out}$, so the critical path will not just be the longest one but we also need a control unit that receiving $C_{out}$ it eventually asserts the shifter load signal. This solution (known as restoring version) can be improved.

\subsection{Restoring and non restoring}

Looking at the previous example, we have to perform comparisons like
$2s^{(1)}-d2^4 <0$. Depending on sign value we decide to store or not the difference result as new partial product, however it may become the bottleneck. Two different approaches can be exploited:

\begin{enumerate}
  \item \textbf{Restoring approach}: from an algebraic point of view in order to restore the previous partial product we can perform:
  $$ 2s^{(1)}-d2^4+2^4d=s^{2}=2s^{(1)}$$
  So in next stage:
  $$ 2s^{(2)}-d2^4=4s^{(1)}-2^4 d$$

  \item \textbf{Non-restoring approach}: we don't test immediately $C_{out}$ but we store the result independently if it negative or not. With respect to previous approach, no additional delay due to carry out propagation is introduced however the result is not correct. The idea is try to correct it in the following step:
  $$2s^{(1)}-d2^4=s^{2}$$
\end{enumerate}

\subparagraph{Non-restoring}
We understand that in the following step we don't perform anymore a subtraction but an addition in the case of negative previous remainder, to compensate the current wrong result.

$$2^{(2)}+2^4 d=4s^{(1)}-2 2^4 d + 2^4 d = 4s^{(1)} -2^4d$$

Since we obtain the same result as in the first approach it means that $C_{out}$ test can be delayed of 1 clock cycle, so we have more time to perform this operation and it is no more the bottleneck. In this way the delay is the one of the adder and control-unit is no more needed, therefore this last implementation leads to a much better design.

In case of negative result at the very last step probably some more computations are required to recover the correct remainder. Since it is the last step of the algorithm, we have no more steps to recover it, meaning that we have to perform a final test on the last step, if the remainder is negative we have to perform an extra-step to compensate the error; in the end each iteration will be faster but maybe an extra step has to be considered, usually this trade off is more than acceptable.

\subparagraph{Example 107/10}

\begin{verbatim}
ex























\end{verbatim}

\section{Signed division}

\subsection{BSD representation}
In a generic step $s^{(j)}$ may be $\geq 0$ so  $q_{k-j}=1$ or $<$, in this case instead of putting $q_{k-j}=0$ we decide to assign the value '-1', in this way usual binary digit set is replaced with ${0,1} \rightarrow {-1, 1}$. Starting from now $q$ value will be represented in this new set, called BSD (\textbf{Binary Signed Digit}).\\

The conversion from BSD to binary can be performed using a 4-steps algorithm:
\begin{enumerate}
  \item Replace all digits equal to "-1" with "0".
  \item Shift one bit to the left.
  \item Complement the MSB.
  \item Add 1 in the least significant position.
\end{enumerate}

As an example let's consider $(11)_{10}$ corresponding to:
\begin{verbatim}
    1 1 -1  1   // start, in BSD 13-2=11
    1 1 0 1   // step 1
 1  1 0 1 -   // step 2
 0  1 0 1 -   // step 3
 0  1 0 1 1   // step 4
\end{verbatim}

The final result is expressed as usual and it is equal to 11. This is useful in signed division.

\subsection{Signed division algorithm}

We expect that $sign(r)=sign(z)$ i.e. the sign of the remainder must be equal to the sign of the dividend and that $sign(q)=sign(z) \oplus sign(d)$. Restoring algorithm can be reused leading to the well known recursive expression:
$$s^{(j)}=2s^{(j-1)}-q_{k-j}d2^k$$

If $sign(s^{(j-1)}) \neq sign(d)$ instead of performing a subtraction we have to perform an additions, in this way we can proceed with same steps as in unsigned case. Depending on divisor sign for each step :
\begin{itemize}
  \item If $d>0$:
    \subitem if $s^{(j-1)} \geq 0$ then $q=1$ and a subtraction has be performed.
    \subitem if $s^{(j-1)} < 0$ then $q=-1$ and an addition has be performed.
  \item If $d<0$:
    \subitem if $s^{(j-1)} \geq 0$ then $q=-1$ and an addition has be performed.
    \subitem if $s^{(j-1)} < 0$ then $q=1$ and a subtraction has be performed.
\end{itemize}

If at the end the sign of partial remainder is not correct again we have to perform an extra step to achieve a correct result for it.\\

So far we have considered unsigned/signed shift and subtraction operations for the division using a iterative approach. How can we achieve a faster division?

\section{Fast division}
Two different approaches can be exploited:
\begin{enumerate}
  \item Same as before but as fast as possible: choose fast adders/subtracters but the algorithm remains iterative.
  \item Try to reduce the number of iterations leading to radix-approach.
\end{enumerate}

So far the amount of required iterations was equal to $k$ since we evaluated 1 bit at the time (radix-2), the idea is to compute 2 bits/iteration in order to obtain ideally a double faster execution (radix-4). It is also possible evaluate 3 bits/iteration (radix-8). In general in order to employ a radix approach we have to substitute in last expression '2' with 'r', leading to:

$$s^{j}=rs^{(j-1)}-q_{k-j} (r^k d)$$

\begin{verbatim}
img46














\end{verbatim}

Bn applying this approach the key architecture is more or less quite similar to the original one. The problem is to guess the correct value for $q_j$ since before it was just 0 or 1, now there are 4 possibilities at each iteration and depending on the remainder sign we have to determine the correct one, in principle we should determine 4 partial products, perform the subtraction and then see which of these 4 has a plus sign. A solution may be simplify the guess of digits by employing an extended digit set.

\subparagraph{Example (base 10)}

\begin{verbatim}
img47














\end{verbatim}
In the first step we take the 2 MSB of z and 1 MSB of d, so $12/2=6$ and although MSB of q is wrong we ignore this fact and keep it, then we proceed with the remainder, we obtain a negative remainder so instead of choose a different value for 6 we actually go on and choose as second digit -3, then we perform the same computations obtaining a positive remainder which is actually the correct one. For the remainder we have $= 6 -3$ which is $6 \cdot 10+(-3) \cdot 1=57$. We decide to accept wrong values during computations since we are using a different digit set (the one that also included negative value).

\subparagraph{Array based solution}

\begin{verbatim}
img48



















\end{verbatim}

In first row the difference between $z$ and $d$ shifted of 4 position is performed since carry in of the first adder is 1 and the exor is complementing $d$. In second row:ì depending on $q_3$ sign we perform an addition or a subtraction.
Array based divider is able to perform the whole shift and subtract algorithm, however it's fully combinational with a huge delay, since each row is actually a RCA. Overall total delay is equal to $k^2 \cdot delay_{single,adder}$ so it is in quadratic relationship with respect to $k$. To speed up the execution is possible to pipeline, in particular if we have to perform a lot of divisions one after the other it is a very good solution, especially for throughput (not for latency since it is very high).

\section{Division by repeated multiplications}
MISS THE LINEAR APPROXIMATION GRAPH!!!

One more additional idea to implement division:

$$q=\frac{z}{d}=\frac{z x^{0} x^{1}... x^{m-1}}{d x^{0} x^{1}... x^{m-1}}$$

The key idea is to multiply $z$ and $d$ by different values, a part for the precision this approach can be employed only if at the end we guarantee that $d x^{0} x^{1}... x^{m-1}$ is equal to 1 then $z x^{0} x^{1}... x^{m-1}$ will be equal to $q$.

The points are:
\begin{itemize}
  \item Number of iterations required.
  \item Sequence of $x$ values guaranteeing the result.
\end{itemize}

By defining $d^{(0)}=d$ then $d^{(i+1)}=d^{(i)}x^{(i)}$ and $z^{(i+1)}=z^{(i)}x^{(i)}$.

We obtain a reasonable result by choosing for $x$ the reciprocal of d (to have at the end $x \cdot d \simeq 1$), in this condition we can approximate $1/d=x$ with its tangent:

\begin{eqnarray}
x^{(i)}=2-d^{(i)}\\
d^{(i+1)}=d^{(i)} x^{(i)}=d^{(i)}(2-d^{(i)})\\
1-d^{(i+1)}=(1-d^{(i)})^2\\
1-d^{(i+1)} < \epsilon -> 1-d^{(i+1)} < \epsilon^2
\end{eqnarray}

This results in an error going down quadratically.

\subparagraph{Example}
$d=$

\begin{eqnarray}
1-d^{(0)} <= 1/2 = 2^{-1}\\
1-d^{(1)} <= 2^{-2}\\
1-d^{(2)} <= 2^{-4}\\
...\\
1-d^{(m-1)} <= 2^{-2^m}
\end{eqnarray}

Assuming to work on $k$ bits then the weight of LSB is equal to $2^{-k}$, by setting the error equal to the weight of LSB is enough since any further iterations would increase the accuracy but it would be useless since we don't have enough bits. This results in:

$$m=log_2 (k)$$

As an example starting from $k=32$, $m=5$ (5 multiplications) are enough. The complexity we have to pay is $m$ multiplications (only the one at numerator, not at denominator since they are not useful for the output result); resulting in a method which is still iterative, some multiplications are required but with a significant lower number of steps.

\section{Division by reciprocation}

Again we want to compute $q=z/d$, focusing on denominator an idea is to compute $1/d$ and then multiply the numerator by this quantity. This approach becomes useful when $d$ is kept constant and $z$ changes. To evaluate $1/d$ multiple possibilities can be employed, one of them is to use Newton-Raphson method (the one for root computation):

\begin{verbatim}
img50














\end{verbatim}

This leads to find successive approximations of the roots basing on previous estimations. By defining:
\begin{eqnarray}
f(x)=1/x -d \\
f'(x)=-1/x^2
\end{eqnarray}

Roots are actually the values we want to find:
$$x^{(i+1)}=x^{(i)}(2-x^{(i)}d)$$

Also here the convergence speed is quadratic, therefore few iterations are needed.

\subsection{Use of look up table}
The following method can be applied to any kind of computation, starting from a memory for each address (which is actually $d$) the output will be $1/d$.

\begin{verbatim}
img51














\end{verbatim}

The complexity of memory increases exponentially with respect to $k$: LUTs are fine only if $w$ is manageable, this implies that $w \leq 16$ otherwise memory size becomes too big.\\

LUTs are nice when we have to compute some particular mathematical expressions (e.g. log, exp, ...), in particular in order to avoid an exponential growth of memory it is a good idea to save just some values and then obtain the missing ones by interpolation, in this way we can limit the amount of saved points. For instance if we just save 4 values in LUT, we can access to the memory using 2 MSB of $x$, memory will return $a,b$ coefficients, and then it is possible to approximate the generic function $f$ like $y=ax+b$ (linear).

\begin{verbatim}
img53














\end{verbatim}

Actually LUT doesn't contain the values of $f$ in specific points but the coefficients $a,b$ that best approximate the function.

\begin{verbatim}
img54














\end{verbatim}

In left region we can notice a highly non linear behavior while in right region the function is almost linear. We can employ the same solution as previous as before (LUT with $a,b$ computed starting from equally distributed samples), or better choosing a non uniform segmentation to have more precise values in the strongly non linear part (multi resolution LUT). This can be obtained by using 2 LUTs.

Depending on the fact that we are or not in the linear part, we choose the coarse or the fine approximation.

So we will a have a CG LUT (Coarse grain) with $2^2=4$ locations and a FG LUT (Fine grain) with $2^4=16$ locations, globally 20 locations are needed.
To implement the same resolution given by FG LUT using only one LUT we would need $2^{4+2}=2^6$ locations which is pretty much. In many cases we can distinguish a linear and a non-linear region to save space.
